{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computing and IT Documentation","text":"<p>This site contains user documentation for those working in the Department of Physics and Astronomy at UCL. Please use the navigation menu on the left-hand side of this page to find the information relevant to you, or type in the search box at the top.</p> <p>Computing and IT issues affecting the department as a whole are handled by the department's Academic Computing Support Team (ACST), overseen by the Departmental IT Committee. Further details are on the departmental intranet on SharePoint under Computing in the Department.</p>"},{"location":"calendar/","title":"Departmental Calendar","text":"<p>Departmental events are included in an Outlook calendar: PhysAst.Events.</p> <p>To include it in your preferred calendar application, you wil need to refer to instructions for the specific application. As an example, if you are using the Outlook Calendar web interface, you can</p> <ul> <li>ensure you are logged in using your UCL account;</li> <li>click \"Add calendar\";</li> <li>select \"Add from directory\", and select your UCL account under \"Please select an account to search from\";</li> <li>type \"PhysAst.Events\" and click the entry that appears;</li> <li>under \"Add to\", select \"Other calendars\" and click \"Add\".</li> </ul> <p>You should now see entries from the departmental calendar alongside those from your own calendar and any others you have added.</p>"},{"location":"contacts/","title":"Contacts","text":"<p>For help with UCL central IT and computing facilities, please contact ISD.</p> <p>For help with departmental systems and services, please contact the appropriate system manager for your research group or area using these e-mail addresses:-</p> Group Contact address HEP support@hep.ucl.ac.uk AMOPP physics-amopp-it@ucl.ac.uk CMMP lcn.it-support@ucl.ac.uk BioP physics-biop-it@ucl.ac.uk All other requests physast.itsupport@ucl.ac.uk <p>Names and contact details for individual staff are available on the departmental intranet under Computing in the Department.</p>"},{"location":"printing/","title":"Printing and Scanning","text":"<p>There are various multifunction devices (MFDs) around the department that can be used to print, copy and scan documents. They are available through the Print@UCL service.</p> <p>Devices are available on each floor of the Physics Building</p> <ul> <li>in the mail room on the ground floor (E)</li> <li>in the print room next to the list on A floor</li> <li>in the corridor on floors D to B</li> </ul> <p>in the North West Wing</p> <ul> <li>room 223</li> <li>Lewis's Building 3rd floor</li> <li>Lewis's Building 4th floor</li> </ul> <p>and in the Cosmoparticle Hub (KLB G23).</p>"},{"location":"software/","title":"Software Licences","text":"<p>In addition to the open-source software available on the department's Linux clusters and users' laptops, some commercial packages are available through licences belonging to the department or to UCL. This page lists some that are particularly relevant for physics research and teaching. MAny others are listed in the UCL Software Database.</p>"},{"location":"software/#comsol-multiphysics","title":"COMSOL Multiphysics","text":"<p>COMSOL Multiphysics is a general-purpose software toolkit for simulating physical systems. The base package provides limited capabilities which can be extended using add-on modules. The software is installed on a PC in the teaching labs, but the department has a floating network licence that allows use of the software on other machines subject to a limit of two simultaneous users. This licence covers</p> <ul> <li>COMSOL Multiphysics (2 concurrent users)</li> <li>AC/DC Module (2 concurrent users)</li> <li>CAD Import Module (1 concurrent user)</li> <li>Material Library (1 concurrent user)</li> <li>Heat Transfer Module (1 concurrent user)</li> </ul> <p>Because this software is limited (because of its cost) to two concurrent users, we do not include full instructions here but you should contact Ben or Fahad at physast.itsupport@ucl.ac.uk if you would like access. You should also close the application when not actively using it so that someone else can use the licence.</p>"},{"location":"software/#mathematica","title":"Mathematica","text":"<p>Mathematica from Wolfram is a popular system for technical computing, and is particularly useful for symbolic mathematical calculations. It is available on Desktop@UCL and can be used on other devices under a UCL site licence: for details see details for Mathematica on the UCL Software Database.</p>"},{"location":"software/#matlab","title":"Matlab","text":"<p>Matlab from MathWorks is a widely used programming and numeric computing platform. It is available on Desktop@UCL and can be used on other devices under a UCL site licence: for details see package details for Matlab on the UCL Software Database.</p>"},{"location":"amopp/","title":"Atomic, Molecular, Optical and Positron Physics (AMOPP)","text":"<p>This documentation is maintained by the AMOPP Computing Group for the purpose of sharing information about our services, including user guides, service updates and account request.</p>"},{"location":"amopp/#documentation","title":"Documentation","text":"<p>General User Information:</p> <ul> <li>Getting an Account</li> <li>Getting started</li> </ul>"},{"location":"amopp/#cluster-information","title":"Cluster Information","text":"<ul> <li>Remote Access</li> </ul> <ul> <li>Storage Explained</li> <li>Storage - Visualised</li> </ul>"},{"location":"amopp/#walkthroughs","title":"Walkthroughs...","text":"<ul> <li>How To...</li> </ul>"},{"location":"amopp/#support","title":"Support","text":"<p>Important</p> <p>Please ensure to include your research group or your PI's name in the email if you know it.</p> <p>For support for any of our services or to report a problem with any of our computing platforms, contact us at: physics-amopp-it@ucl.ac.uk</p> <p>We will endeavour to answer queries on any aspect of computing related to your research as soon as possible.</p>"},{"location":"amopp/account-services/","title":"Account Services","text":""},{"location":"amopp/account-services/#cluster-access","title":"Cluster Access","text":"<p>Important</p> <p>Please ensure you know you which group you are a part of. Alternatively, please ensure you have permission to request an account by including a permanent member of staff (normally your supervisor or PI) to the email.</p> <p>Please email: physics-amopp-it@ucl.ac.uk to request an account.</p>"},{"location":"amopp/getting-started/","title":"Getting Started","text":"<p>Important</p> <p>Please ensure you have an account. To request an account, please follow the instructions here Getting an Account.</p>"},{"location":"amopp/getting-started/#remote-access-to-cluster-resources","title":"Remote Access to Cluster Resources","text":"<p>Computing services are accessible from inside the UCL however, if you wish to connect from outside UCL, you need to either connect via the UCL VPN or use SSH to log in to the Gateway node that is accessible from the outside and use that to \"jump\" to the required resource.</p>"},{"location":"amopp/getting-started/#connecting-to-the-gateway-node","title":"Connecting to the gateway node","text":"<p>You can connect to the jump boxes by connecting with your SSH client to:</p> <pre><code>ssh0.theory.phys.ucl.ac.uk\n</code></pre> <p>Once connected you can then log on to the node you wish to use.</p>"},{"location":"amopp/howto/","title":"How do I?","text":"<p>I have an account, now:</p>"},{"location":"amopp/howto/#how-do-i-log-in","title":"How do I log in?","text":"<p>Logging in is most straightforward if you are inside the UCL firewall. If you are logging in from home or other external networks then you first have to get on to the UCL network.</p>"},{"location":"amopp/howto/#linux-unix-macos","title":"Linux / Unix / macOS","text":"<p>Use the terminal and type the below command to secure shell (ssh) into the machine you wish to access. Replace <code>&lt;username&gt;</code> with your username, and <code>&lt;node&gt;</code> with the name of the machine you want to log in to, eg. <code>angus</code>, <code>zed</code>, <code>butch</code>. </p> <pre><code>ssh &lt;username&gt;@&lt;node&gt;.theory.phys.ucl.ac.uk\n</code></pre>"},{"location":"amopp/howto/#windows","title":"Windows","text":"<p>On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, or on Windows 10 you can use OpenSSH from a command prompt and type the same <code>ssh</code> command as the  Linux instructions.</p>"},{"location":"amopp/howto/#using-putty","title":"Using PuTTY","text":"<p>PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under:  <code>Start &gt; P &gt; PuTTY 0.83 (64-bit) &gt; PuTTY 0.83</code> or type \"putty\" in the toolbar's search box.</p> <p>You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. </p> <p></p> <p>You will then be asked to enter your username and password. Only enter your username, not <code>@&lt;node&gt;.theory.phys.ucl.ac.uk</code>. The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something. </p> <p>The first time you log in to a new server, you'll get a popup telling you that the server's host  key is not cached in the registry - this is normal and is because you have never connected to this server before. If you want to, you can check the host fingerprint against our current key fingerprints.</p>"},{"location":"amopp/howto/#logging-in-from-outside-the-ucl-firewall","title":"Logging in from outside the UCL firewall","text":"<p>You will need to either use the UCL Virtual Private Network or ssh in to UCL's Gateway system <code>ssh0.theory.phys.ucl.ac.uk</code> first. From there you can then ssh in to our systems. </p> <pre><code>ssh &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\nssh &lt;username&gt;@&lt;node&gt;.theory.phys.ucl.ac.uk\n</code></pre> <p>Advanced: If you find you need to go via the Gateway often, you can set up this jump automatically, see Single-step logins using tunnelling</p>"},{"location":"amopp/howto/#login-problems","title":"Login problems","text":"<p>If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you have recently updated your password, it takes some hours to propagate to all UCL systems.</p> <p>If you still cannot get access but can access other UCL services like the SSH Gateway, please contact us on rc-support@ucl.ac.uk. Your account may have expired, or you may have gone over quota.</p> <p>If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk. </p> <p>If you get a host key error message, you will need to delete old host keys - continue reading!</p>"},{"location":"amopp/howto/#remote-host-identification-has-changed","title":"Remote host identification has changed","text":"<p>When you log in via SSH, it keeps a record of the host key for the server you logged in to in  your <code>.ssh/known_hosts</code> file in your home directory, on the machine you are logging in from.  This helps make sure you are connecting directly to the server you think you are, but can cause  warnings to show up if the host key on that machine has genuinely changed (usually because of an  update or reinstall).</p> <p>Check the host key warning against our current key fingerprints:</p> <p>The error message looks like this if you are using OpenSSH in a terminal: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA.\nPlease contact your system administrator.\nAdd correct host key in /Users/uccaxxx/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /Users/uccaxxx/.ssh/known_hosts:11\nECDSA host key for angus.theory.phys.ucl.ac.uk has changed and you have requested strict checking.\nHost key verification failed.\nKilled by signal 1.\n</code></pre> This tells you that the old key is in line 11 of your <code>known_hosts</code> file.  Sometimes it will give you a direct command you can run to remove that specific key: <pre><code>ssh-keygen -R angus.theory.phys.ucl.ac.uk\n</code></pre> or you can manually delete line 11 yourself in a text editor.</p> <p>If you are logging in via the Gateway, you will need to remove the old key there too. On the Gateway,  <code>nano</code> and <code>vim</code> are available text editors. If you are not already familiar  with <code>vim</code>, use <code>nano</code> - it has the command shortcuts shown at the bottom, where <code>^O</code> means  press <code>Ctrl</code> and then the letter <code>o</code>. <pre><code># to open the file for editing in nano\nnano ~/.ssh/known_hosts\n</code></pre> Once you have removed the old host key you will be able to ssh in again. The first time  you log in to an unknown server you will get a message like this: <pre><code>The authenticity of host 'angus.theory.phys.ucl.ac.uk can't be established.\nECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> Typing <code>yes</code> will allow you to continue logging in.</p> <p>PuTTY will display a warning and you can choose to continue or not.</p> <p>WinSCP will say <code>Server's host key does not match the one that WinSCP has in cache.</code>  and you will have the option to update the key.</p>"},{"location":"amopp/howto/#macos-connection-failures","title":"MacOS connection failures","text":"<p>If you are on macOS and getting many ssh connection failures and broken pipe messages when trying to log in, try adding an ssh timeout to your ssh command: <pre><code>ssh -o ConnectTimeout=10 &lt;your_UCL_user_id&gt;@angus.theory.phys.ucl.ac.uk\n</code></pre> This has particularly been a problem with macOS Big Sur when using the VPN.</p>"},{"location":"amopp/howto/#how-do-i-log-out","title":"How do I log out?","text":"<p>You can log out of the systems by typing <code>exit</code> and pressing enter.</p> <p>(<code>logout</code> or pressing Ctrl+D also work)</p>"},{"location":"amopp/howto/#how-do-i-transfer-data-onto-the-system","title":"How do I transfer data onto the system?","text":"<p>icon: material/ghost You can transfer data to and from our systems using any program capable of using the Secure Copy (SCP) protocol. This uses the same SSH system as you use to log in to a command line session, but then transfers data over it. This means that if you can use SSH to connect to a system, you can usually use SCP to transfer files to it. </p>"},{"location":"amopp/howto/#copying-files-using-linux-or-macos","title":"Copying files using Linux or macOS","text":"<p>You can use the command-line utilities scp, sftp or rsync to copy your data about. You can also use a graphical client (Transmit, CyberDuck, FileZilla).</p>"},{"location":"amopp/howto/#scp","title":"Scp","text":"<p>This will copy a data file from somewhere on your local machine to a specified location on the  remote machine (Butch etc).</p> <p><pre><code>scp &lt;local_data_file&gt; &lt;username&gt;@&lt;remote_hostname&gt;:&lt;remote_path&gt;\n</code></pre> <pre><code># Example: copy myfile from your local current directory into Scratch on Butch\nscp myfile &lt;username&gt;&gt;@angus.theory.phys.ucl.ac.uk:/scratch/&lt;username&gt;\n</code></pre></p> <p>This will do the reverse, copying from the remote machine to your local machine. (This is still run from your local machine).</p> <p><pre><code>scp &lt;username&gt;@&lt;remote_hostname&gt;:&lt;remote_path&gt;&lt;remote_data_file&gt; &lt;local_path&gt;\n</code></pre> <pre><code># Example: copy myfile from any node into the Backups directory in your local current directory\nscp ccxxxxx@ssh0.theory.phys.ucl.ac.uk:/scratch/&lt;username&gt;/myfile Backups/\n</code></pre></p>"},{"location":"amopp/howto/#sftp","title":"Sftp","text":"<p>You can use <code>sftp</code> to log in to the remote machine, navigate through directories and use <code>put</code> and <code>get</code> to copy files from and to your local machine. <code>lcd</code> and <code>lls</code> are local equivalents of <code>cd</code> and <code>ls</code> so you can navigate through your local directories as you go. </p> <p><pre><code>sftp &lt;username&gt;@&lt;remote_hostname&gt;\ncd &lt;remote_path&gt;\nget &lt;remote_file&gt;\nlcd &lt;local_path&gt;\nput &lt;local_file&gt;\n</code></pre> <pre><code># Example: download a copy of file1 into your local current directory,\n# change local directory and upload a copy of file2\nsftp ccxxxxx@ssh0.theory.phys.ucl.ac.uk\ncd Scratch/files\nget file1\nlcd ../files_to_upload\nput file2\n</code></pre></p>"},{"location":"amopp/howto/#rsync","title":"Rsync","text":"<p><code>rsync</code> is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at <code>man rsync</code> as there are many options. </p>"},{"location":"amopp/howto/#copying-files-using-windows-and-winscp","title":"Copying files using Windows and WinSCP","text":"<p>WinSCP is a graphical client that you can use for <code>scp</code> or <code>sftp</code>.</p> <ol> <li>The login/create new session screen will open if this is the first time you are using WinSCP.</li> <li>You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better.</li> <li>Fill in the hostname of the machine you wish to connect to, your username and password.</li> <li>Click Save and give your settings a useful name.</li> <li>You'll then be shown your list of Stored sessions, which will have the one you just created.</li> <li>Select the session and click Login.</li> </ol>"},{"location":"amopp/howto/#mobaxterm","title":"MobaXterm","text":"<p>If using MobaXterm, you may need to set a password for the left side file manager  panel separately as well as for the main panel, to allow you to drag and drop  files and have them transferred to the cluster.</p>"},{"location":"amopp/howto/#transferring-files-from-outside-the-ucl-firewall","title":"Transferring files from outside the UCL firewall","text":"<p>As when logging in, when you are outside the UCL firewall you will need a method to connect inside it before you copy files.</p> <p>You can use the UCL Virtual Private Network and scp direct to our systems or you can do some form of SSH tunnelling.</p>"},{"location":"amopp/howto/#single-step-logins-using-tunnelling","title":"Single-step logins using tunnelling","text":""},{"location":"amopp/howto/#linux-unix-macos_1","title":"Linux / Unix / macOS","text":""},{"location":"amopp/howto/#on-the-command-line","title":"On the command line","text":"<p><pre><code># Log in to Angus, jumping via the Gateway (replace ccxxxxx with your own username)\nssh -o ProxyJump=&lt;username&gt;@angus.theory.phys.ucl.ac.uk ccxxxxx@ssh0.theory.phys.ucl.ac.uk\n</code></pre> or <pre><code># Copy 'my_file' from the machine you are logged in to into your Scratch on Grace\n# Replace ccxxxxx with your own username.\nscp -o ProxyJump=&lt;username&gt;@angus.theory.phys.ucl.ac.uk my_file &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk:/scratch/&lt;username&gt;\n</code></pre></p> <p>This tunnels through the Gateway in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files.</p> <p>You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.</p>"},{"location":"amopp/howto/#using-a-config-file","title":"Using a config file","text":"<p>You can create a config file which does this without you needing to type it every time.</p> <p>Inside your <code>~/.ssh</code> directory on your local machine, add the below to your <code>config</code> file (or create a file called <code>config</code> if you don't already have one).</p> <p>Generally, it should be of this form where <code>&lt;name&gt;</code> can be anything you want to call this entry.</p> <p><pre><code>Host &lt;name&gt;\n   User &lt;username&gt;\n   HostName &lt;remote_hostname&gt;\n   proxyCommand ssh -W &lt;remote_hostname&gt;:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre> This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host.</p> <p>Here are some examples - you can have as many of these as you need in your config file. <pre><code>Host angus\n   User ccxxxxx\n   HostName angus.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W angus.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n\nHost zed\n   User ccxxxxx\n   HostName zed.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W zed.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n\nHost butch\n   User ccxxxxx\n   HostName butch.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W butch.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre></p> <p>You can now just type <code>ssh ssh0</code> or <code>scp file1 ssh0:~</code> and you will go through the Gateway. You'll be asked for login details twice since you're logging in to two machines, a Gateway server and your endpoint.  </p>"},{"location":"amopp/howto/#windows-winscp","title":"Windows - WinSCP","text":"<p>WinSCP can also set up SSH tunnels.</p> <ol> <li>Create a new session as before, and tick the Advanced options box in the bottom left corner.</li> <li>Select Connection &gt; Tunnel from the left pane.</li> <li>Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example ssh0.theory.phys.ucl.ac.uk</li> <li>Fill in your username and password for that host. (Central UCL ones for the Gateway).</li> <li>Select Session from the left pane and fill in the hostname you want to end up on after the tunnel.</li> <li>Fill in your username and password for that host and set the file protocol to SCP.</li> <li>Save your settings with a useful name.</li> </ol>"},{"location":"amopp/howto/#creating-a-tunnel-that-other-applications-can-use","title":"Creating a tunnel that other applications can use","text":"<p>Some applications do not read your SSH config file and also cannot set up tunnels themselves, but can use one that you have created separately. FileZilla in particular is something you may want to do this with to transfer your files directly to the clusters from outside UCL using  a graphical client.</p>"},{"location":"amopp/howto/#ssh-tunnel-creation-using-a-terminal","title":"SSH tunnel creation using a terminal","text":"<p>You can do this in Linux, macOS and the Windows Command Prompt on Windows 10 and later.</p> <p>Set up a tunnel between a port on your local computer (this is using 3333 as it is unlikely to be in use, but you can pick different ones) to Myriad's port 22 (which is the standard port for ssh),  going via a UCL gateway.</p> <pre><code># replace ccxxxxx with your Cluster username\nssh -L 3333:angus.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk \n</code></pre> <p>You may also want to use the <code>-N</code> option to tell it not to execute any remote commands and  <code>-f</code> to put this command into the background if you want to continue to type other commands  into the same terminal.</p> <p>The tunnel now exists, and <code>localhost:3333</code> on your computer connects to Myriad.</p> <p>You can do this with ports other than 22 if you are not wanting to ssh in but to instead connect with a local browser to something running on Angus. Here the port remains as 3333, something could be launched on that port on Angus and your browser could be pointed at  <code>localhost:3333</code> to connect to it.</p> <pre><code># replace ccxxxxx with your UCL username\nssh -L 3333:angus.theory.phys.ucl.ac.uk:3333 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre> <p>Do not leave things like this running for long periods on the login nodes.</p>"},{"location":"amopp/howto/#ssh-tunnel-creation-using-putty","title":"SSH tunnel creation using PuTTY","text":"<p>On Windows you can also set up a tunnel using PuTTY.</p>"},{"location":"amopp/howto/#connect-to-your-tunnel-with-an-application-like-filezilla","title":"Connect to your tunnel with an application (like FileZilla)","text":"<p>You can then tell your application to connect to <code>localhost:3333</code> instead of Angus. If it has  separate boxes for hostname and port, put <code>localhost</code> as the hostname and <code>3333</code> as the port.</p>"},{"location":"amopp/howto/#managing-your-quota","title":"Managing your quota","text":"<p>After using <code>lquota</code> to see your total usage, you may wish to find what is using all your space.</p> <p><code>du</code> is a command that gives you information about your disk usage. Useful options are:</p> <pre><code>du -ch &lt;dir&gt;\ndu -h --max-depth=1\n</code></pre> <p>The first will give you a summary of the sizes of directory tree and subtrees inside the directory you specify, using human-readable sizes with a total at the bottom. The second will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage.</p>"},{"location":"amopp/howto/#how-do-i-connect-out-to-an-ftp-server","title":"How do I connect out to an FTP server?","text":"<p>You cannot connect in to Angus using FTP (we only allow SFTP access) but you can connect out  to FTP servers run by other people. </p> <p>Load the GNU inetutils module which provides ftp, telnet and tftp clients.</p> <pre><code>module load inetutils/1.9.4\n\n# connect to your desired server\nftp servername.ac.uk\n</code></pre> <p>You can then use <code>put</code> and <code>get</code> commands to put data on the remote FTP server or download it from there to Angus. </p>"},{"location":"amopp/howto/#how-do-i-run-a-graphical-program","title":"How do I run a graphical program?","text":"<p>To run a graphical program on the cluster and be able to view the user interface on your own  local computer, you will need to have an X-Windows Server installed on your local computer and  use X-forwarding.</p>"},{"location":"amopp/howto/#x-forwarding-on-linux","title":"X-forwarding on Linux","text":"<p>Desktop Linux operating systems already have X-Windows installed, so you just need to ssh in with the correct flags.</p> <p>You need to make sure you use either the <code>-X</code> or <code>-Y</code> (look at <code>man ssh</code> for details) flags on all ssh commands you run to establish a connection to the cluster.</p> <p>For example, connecting from outside of UCL: <pre><code>ssh -X &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre> and then <pre><code>ssh -X &lt;username&gt;@angus.theory.phys.ucl.ac.uk\n</code></pre></p> <p>A video walkthrough of running remote applications using X11, X-forwarding on compute nodes.</p>"},{"location":"amopp/howto/#x-forwarding-on-macos","title":"X-forwarding on macOS","text":"<p>You will need to install XQuartz to provide an X-Window System for macOS. (Previously known as X11.app).</p> <p>You can then follow the Linux instructions using Terminal.app.</p>"},{"location":"amopp/howto/#x-forwarding-on-windows","title":"X-forwarding on Windows","text":"<p>You will need:</p> <ul> <li>An SSH client; e.g., PuTTY</li> <li>An X server program; e.g., Exceed, Xming</li> </ul> <p>Exceed is available on Desktop@UCL machines and downloadable from the UCL software database. Xming is open source (and mentioned here without testing).</p>"},{"location":"amopp/howto/#exceed-on-desktopucl","title":"Exceed on Desktop@UCL","text":"<ol> <li>Load Exceed. You can find it under Start &gt; All Programs &gt; Applications O-P &gt; Open Text Exceed 14 &gt; Exceed</li> <li>Open PuTTY (Applications O-P &gt; PuTTY)</li> <li>In PuTTY, set up the connection with the host machine as usual:<ul> <li>Host name: <code>angus.theory.phys.ucl.ac.uk</code> (for example)</li> <li>Port: <code>22</code></li> <li>Connection type: <code>SSH</code></li> </ul> </li> <li>Then, from the Category menu, select Connection &gt; SSH &gt; X11 for 'Options controlling SSH X11 forwarding'.<ul> <li>Make sure the box marked 'Enable X11 forwarding' is checked.</li> </ul> </li> <li>Return to the session menu and save these settings with a new identifiable name for reuse in future.</li> <li>Click 'Open' and login to the host as usual</li> <li>To test that X-forwarding is working, try running <code>nedit</code> which is a text editor in our default modules.</li> </ol> <p>If <code>nedit</code> works, you have successfully enabled X-forwarding for graphical applications.</p>"},{"location":"amopp/howto/#installing-xming","title":"Installing Xming","text":"<p>Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working.</p> <ol> <li>Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client.</li> <li>Open Xming - the Xming icon should appear on the task bar.</li> <li>Open PuTTY</li> <li>Set up PuTTY as shown in the Exceed section.</li> </ol>"},{"location":"amopp/remote-access/","title":"Remote Access to Research Computing Resources","text":"<p>UCL's Research Computing services are accessible from inside the UCL firewall.  If you wish to connect from outside, you need to either connect through a VPN or use SSH to log in to a machine accessible from outside and use that to \"jump\" through into the UCL network.</p>"},{"location":"amopp/remote-access/#connecting-to-the-jump-boxes","title":"Connecting to the jump boxes","text":"<p>You can connect to the jump boxes by connecting with your SSH client to:</p> <pre><code>ssh0.theory.phys.ucl.ac.uk\n</code></pre> <p>Once connected you can then log on to the node you wish to use as normal.</p> <p>You can configure your ssh client to automatically connect via these jump boxes so that you make the connection in one step.</p>"},{"location":"amopp/remote-access/#single-step-logins-using-tunnelling","title":"Single-step logins using tunnelling","text":""},{"location":"amopp/remote-access/#linux-unix-macos","title":"Linux / Unix / macOS","text":""},{"location":"amopp/remote-access/#on-the-command-line","title":"On the command line","text":"<p><pre><code># Log in to Butch, jumping via jump box\n# Replace ccxxxxx with your own username.\nssh -o ProxyJump=ccxxxxx@ssh0.theory.phys.ucl.ac.uk ccxxxxx@angus.theory.phys.ucl.ac.uk\n</code></pre> or <pre><code># Copy 'my_file', from the machine you are logged in to, into your Scratch on Kathleen\n# Replace ccxxxxx with your own username.\nscp -o ProxyJump=ccxxxxx@ssh0.theory.phys.ucl.ac.uk my_file ccxxxxx@angus.theory.phys.ucl.ac.uk:/Scratch/&lt;username&gt;\n</code></pre></p> <p>This tunnels through the jump box service in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files.</p> <p>You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.</p>"},{"location":"amopp/remote-access/#using-a-config-file","title":"Using a config file","text":"<p>You can create a config which does this without you needing to type it every time.</p> <p>Inside your <code>~/.ssh</code> directory on your local machine, add the below to your <code>config</code> file (or create a file called <code>config</code> if you don't already have one).</p> <p>Generically, it should be of this form where <code>&lt;name&gt;</code> can be anything you want to call this entry. You can use these as short-hand names when you run <code>ssh</code>.</p> <p><pre><code>Host &lt;name&gt;\n   User &lt;username&gt;\n   HostName &lt;remote_hostname&gt;\n   proxyCommand ssh -W &lt;remote_hostname&gt;:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre> This <code>proxyCommand</code> option causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host.</p> <p>On newer versions of OpenSSH, you can use <code>ProxyJump &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk</code>  instead of this <code>proxyCommand</code> line.</p> <p>Here are some examples - you can have as many of these as you need in your config file. <pre><code>Host angus\n   User ccxxxxx\n   HostName angus.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W angus.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n\nHost zed\n   User ccxxxxx\n   HostName zed.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W zed.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n\nHost butch\n   User ccxxxxx\n   HostName butch.theory.phys.ucl.ac.uk\n   proxyCommand ssh -W butch.theory.phys.ucl.ac.uk:22 &lt;username&gt;@ssh0.theory.phys.ucl.ac.uk\n</code></pre></p> <p>You can now just type <code>ssh angus</code> or <code>scp file1 zed:~</code> and you will go through the jump box. You'll be asked for login details twice since you're logging in to two machines, the jump box and your endpoint.  </p>"},{"location":"amopp/extras/Hostkeys/","title":"Hostkeys","text":"<p>These are the current hostkey fingerprints for our clusters. The are different types of hostkeys (ECDSA, ED25519, RSA) and which one is used depends on your ssh client and its configuration. The MD5 or SHA256  at the front is letting you know what type of fingerprint it is - your ssh client  may not include that part in its output.</p>"},{"location":"amopp/extras/Hostkeys/#angus","title":"Angus","text":"<pre><code>ECDSA    MD5:db:06:ca:12:38:23:1f:12:ed:47:4f:51:0f:19:5d:23\nECDSA    SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:29:a7:45:04:83:86:ec:95:fa:25:dc:7a:f4:93:78:c1\nRSA      SHA256:8H13PdcJGJJbV/F3oBYXX7W1/8q/7m3gLBc8uZP3wio\n</code></pre>"},{"location":"amopp/extras/Hostkeys/#butch","title":"Butch","text":"<pre><code>ECDSA    MD5:6c:94:b2:01:c3:2f:58:5b:f3:03:02:cf:0f:ac:a0:d2\nECDSA    SHA256:rCKAb0yOWXK8+GClKy/pdbwrUbrGMvFkMciZLVcbaTA\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:5a:cf:95:a2:e4:05:8a:36:46:dc:65:0a:f2:8b:ab:e1\nRSA      SHA256:4SMOyhe/MVQ8aUOZyPfHjrIU5zHh7ZhmVd4zxzY+ukI\n</code></pre>"},{"location":"amopp/extras/Hostkeys/#rhoads","title":"Rhoads","text":"<pre><code>ECDSA    MD5:60:13:a6:4d:09:33:4d:67:1b:46:24:ee:44:66:71:17\nECDSA    SHA256:3zwMU9C8d9rgmYJ9qDElo15NnWyF2I4xy2X/VIAmFdo\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:06:17:f3:f2:0c:3e:0d:df:1d:04:fb:53:dc:77:60:56\nRSA      SHA256:DPcjbsUTBq3LwRggu4N+q2WQR0rkoM42jRdYXJtB86M\n</code></pre>"},{"location":"amopp/extras/Hostkeys/#zed","title":"Zed","text":"<pre><code>ECDSA    MD5:11:98:2e:c2:da:14:0c:d3:4e:a3:70:11:e1:59:72:7e\nECDSA    SHA256:3PMLXp6ny0dECycvx4D7+t0sNgsSsLvSO5QUYmzkbhs\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:85:31:4b:cf:1a:ec:64:e4:b2:98:28:4a:46:b2:c1:90\nRSA      SHA256:/qv4BAS9ga6C6iMwj8coEPQGg740CmazeDTFnXeGX+c\n</code></pre>"},{"location":"amopp/storage/","title":"Storage Explained","text":"<p>There are five types of storage for use on all nodes except for gateway nodes (sometimes referred to as jump boxes) which only have access to the home directory.</p> Home Directory <p>Located in /home/{username}</p> <p>This directory is your home folder. All your user files, personal settings, customizations will be saved in this folder. This folder is network aware and will travel with you regardless of which node you connect to.</p> <p>Warning</p> <p>Do not store personal files (non-work/research related content) on these systems such as your music, personal photos/videos etc.</p> Scratch <p>Located in /scratch</p> <p>This is located on every compute node. The purpose of this space is to be used to store running/active simulations temp data. If you need to store the results of the processed data, please consider moving it to either void for durations up to six (6) months or the silos for long term storage. </p> <p>Warning</p> <p>Data stored here will also be moved to #Void storage if unaccessed for a period of 30 days.</p> Void Store <p>Located in /nfs/void</p> <p>This will act as network based scratch storage. This space has very little redundancy and will act as a funnel for data removed from node scratch space. Data will stored for a minimum of six (6) months before it flagged for deletion.</p> Silos <p>Located in /nfs/silo</p> <p>This is protected raid storage. This is to be used for long term storage of active projects. Data stored in this space is not subject to any system action, however any data inactive for a period of two (2) years will be flagged and marked for review by the group/project managers/Pi.</p> Projects Folder <p>Located in /nfs/projects</p> <p>This is protected raid storage. This will be used for archived projects. This space, data and its usage will be at the sole direction of the project managers/PI's.</p>"},{"location":"amopp/theory/storage-map/","title":"Storage Map","text":"<pre><code>graph LR\n  A[Node] --&gt;|3 Months| B[Scratch];\n  B --&gt;|Temp| C[Scratch];\n  B --&gt;|Temp -&gt; | C[Void];\n  B --&gt;|6-12 Months| C[Void];\n  B --&gt;|Permanent| D[Silos];\n  B --&gt;|Permanent| E[Project Moon];\n  B[Scratch] --&gt; F{Delete/Bin?};</code></pre>"},{"location":"astro/","title":"Astrophysics","text":"<p>Information for members of the Astrophysics group can be found on the Astrophysics site in SharePoint. If you don't already have access you will have to request this when prompted.</p> <p>For IT support in the Astrophysics group please contact John Deacon or Edd Edmondson.</p>"},{"location":"av/","title":"AV Equipment and Meeting Support","text":"<p>Information about available meeting equipment and guidance for hybrid meetings is available on the departmental intranet: AV Equipment &amp; Meeting Support.</p>"},{"location":"biop/","title":"Biological Physics (BioP)","text":"<p>Welcome to the BioP Page.</p>"},{"location":"biop/#biop-docs","title":"Biop Docs","text":"<p>Welcome to the BioP Documentation page. </p> <p>For general queries or help with purchasing, software, hardware acquisition or general help with research projects, please email physics-biop-it@ucl.ac.uk. </p>"},{"location":"biop/#cluster-use","title":"Cluster Use","text":"<p>BIOP users mostly use the Dias Cluster. Please refer to the Dias Documentation.</p>"},{"location":"clusters/","title":"Clusters","text":"<ul> <li>Hypatia</li> <li>DIAS</li> <li>Research group clusters</li> <li>ARC Clusters (UCL Centre for Advanced Research Computing)</li> </ul>"},{"location":"clusters/arc/","title":"Central UCL Clusters (ARC)","text":"<p>UCL's Advanced Research Computing Centre (ARC) hosts a range of computating facilities including several large clusters, documented on the  UCL Research Computing Documentation web site.</p> <ul> <li>Myriad is designed for high I/O, high throughput jobs that will run within a single node rather than multi-node parallel jobs.</li> <li>Kathleen is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node.</li> <li>Young is the compute cluster for the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling.</li> <li>Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, delivering computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology.</li> </ul>"},{"location":"clusters/dias/","title":"DIAS","text":""},{"location":"clusters/dias/#overview","title":"Overview","text":"<p>The DIAS cluster offers access to a small number of powerful CPU and GPU systems. It is available to researchers in Physics and Astronomy who need more powerful resources than their own desktop or laptop computers, but don't need low-latency networking for multi-node parallel computation (for which the Hypatia cluster may be more suitable) or the larger clusters provided by ARC or national facilities. It is available to masters (MSc and MSci) students undertaking research projects, where requested by the student's supervisor.</p>"},{"location":"clusters/dias/#account-creation-and-support","title":"Account creation and support","text":"<p>To request an account on DIAS, please e-mail phy.dias.support@ucl.ac.uk with the following information:</p> <ul> <li>your name;</li> <li>your UCL computing ID (e.g. ucapxxx);</li> <li>your research group affiliation (Astro, AMOPP, BioP, CMMP, HEP).</li> </ul> <p>UCL email addresses (i.e. of style ..@ucl.ac.uk) are mandatory, as no account information will be sent to non-UCL email addresses. <p>For masters students, the request must be made by the project supervisor or course leader, and should also include:</p> <ul> <li>supervisor's name;</li> <li>supervisor's UCL computing ID (e.g. ucapxxx);</li> <li>supervisor's UCL e-mail address.</li> </ul> <p>Masters students should also contact their supervisor in the first instance with any queries or problems relating to the DIAS cluster. The supervisor can decide if a problem requires system administrator support.</p>"},{"location":"clusters/dias/#access","title":"Access","text":"<p>Access to DIAS is by SSH to dias.hpc.phys.ucl.ac.uk, e.g. <code>ssh username@dias.hpc.phys.ucl.ac.uk</code>. DIAS access requires that you be within the UCL network. </p> <p>If you are not on site at UCL you can still access the cluster by using </p> <ul> <li>the UCL VPN);</li> <li>Desktop@UCL Anywhere;</li> <li>the SSH gateway.</li> </ul> <p>If you encounter any issue or need assistance with these services you will need to contact the ISD Service Desk.</p> <p>For running graphical applications remotely from a Windows machine you will need an X server application. We recommend you install OpenSSH on your Windows machine. This is the easiest solution to connecting to SSH via windows as it is also a native application on Unix systems.</p> <p>Alternatively you can find programs such as Exceed, which can be downloaded from the UCL software database: http://swdb.ucl.ac.uk/package/view/id/150 and an SSH client that supports X11 forwarding (e.g. PuTTY: https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html).</p> <p>This video tutorial may be useful for Desktop@UCL users: Connecting to a Linux system from Desktop@UCL.</p>"},{"location":"clusters/dias/#using-the-cluster","title":"Using the cluster","text":"<p>The DIAS cluster runs a variant of the CentOS 7 Linux operating system. Using Linux remotely you will mainly use a terminal interface running \u2018bash\u2019.</p> <p>If you are new to Linux or Research computing you can find some helpful online courses and resources provided by UCL ARC in the ARC Course Catalogue.</p> <p>If you are looking for a more casual approach to learning how to use a Linux system you can find many websites with useful commands, examples and tutorials.</p> <p>Please note that data on DIAS is not backed up and you should keep code in external source control and otherwise keep copies of data elsewhere.</p> <p>DIAS operates in the style of an HPC cluster. HPC clusters differ somewhat in how they are used from conventional computing, in that the sorts of large or processor intensive jobs used on them must be submitted to a job manager so that appropriate resources are allocated and do not conflict with other jobs.</p> <p>The node you log in to is the login node. In the case of Dias this is also the head node which manages the cluster. It's important for other users and the cluster as a whole that high memory or processor intensive jobs are never run on this node. If such jobs are found to be running they will be terminated without warning. You are encouraged to run normal editing and development work on the login node however, so you can edit files, download data, check your work into repositories, move files around and so on without a problem.</p> <p>When you come to run intensive jobs, you must request a job from the job manager, which in the case of Dias is software called Slurm (Simple Linux Utility for Resource Management). This software will allocate your job to one of the two CPU nodes, or the GPU node. If Dias is busy there may be a wait for resources, but Slurm tries to allocate resources fairly so that users get an appropriate share of the time.</p> <p>Resources are divided into partitions which contain different types of hardware - for Dias these are normal CPU nodes in the COMPUTE partition, one GPU node in the GPU partition, and one GPU node in the LIGHTGPU partition.</p>"},{"location":"clusters/dias/#submitting-a-job-to-slurm","title":"Submitting a job to Slurm","text":"<p>To submit a job you must write a bash script that has the commands needed to run your code, and also includes directives to Slurm for what resources are needed. This script is then submitted using the sbatch command. The directives to Slurm are special comment lines beginning with #SBATCH. </p> <p>In the example below these are interleaved with normal explanatory comments. This can be difficult to understand, so don't hesitate to ask for help if needed.</p> <p>An example job script might look like <pre><code>#!/bin/bash\n#submit to the normal COMPUTE partition for normal CPUS\n#SBATCH -p COMPUTE\n#request a different amount of time to the default 12h\n#SBATCH --time 24:00:00\n#requesting one node\n#SBATCH -N1\n#requesting 12 cpus\n#SBATCH -n12\n#SBATCH --mail-user=youremail@ucl.ac.uk\n#SBATCH --mail-type=ALL\ncd /home/username/programlocation\nsrun ./program \n</code></pre></p> <p>The job is submitted with 'sbatch scriptname' and you will be given a job number by Slurm. The srun command is a special part of Slurm that unlocks multiple processors for MPI code. There are different technologies for different kinds of multiple processor use, so if your code uses multiple processors you may want to contact technical support to make sure you are doing so correctly. Your supervisor may be able to advise on whether the code is for example OpenMP, MPI, or uses some other method, and providing this information to us will help us get your code running well.</p>"},{"location":"clusters/dias/#gpu-requests","title":"GPU requests","text":"<p>A request for a GPU node must also request one or more of its NVIDIA A100 cards. This is done with something like <pre><code>#!/bin/bash\n#SBATCH -p GPU\n#SBATCH --gres=gpu:a100:1\n</code></pre> The 1 at the end of the line requests 1 card. More can be requested but these are valuable resources and you should be sure your code makes good use of multiple cards and will not run for too long blocking out other users. Resources for CPUs can be requested still with the -n option. The GPU partition has 3 40GB A100 cards available. The LIGHTGPU partition (selected by using #SBATCH -p LIGHTGPU ) has the same hardware but divided into 6 20GB instances. If you can use the LIGHTGPU partition instead, please do, but note that you must only request one GPU per job in this partition.</p> <p>LIGHTGPU users may need to switch the first line of their script to <pre><code>#!/bin/bash -l\n</code></pre> in order to correctly have the CUDA_VISIBLE_DEVICES variable set, or alternatively add <pre><code>source /etc/slurm/gpu_variables.sh\n</code></pre> to your job script at the start.</p>"},{"location":"clusters/dias/#interactive-jobs","title":"Interactive jobs","text":"<p>A simple interactive job can be run by using srun in a different mode to call a shell. srun will make the necessary resource request, and you do not need to use the sbatch command in this case. <pre><code>srun --spankx11 --pty bash\n</code></pre> Further arguments can be used to request additional resources beyond just one processor, e.g. 'srun -N1 -n2 --pty bash' for two processors on one node.</p>"},{"location":"clusters/dias/#other-resources","title":"Other resources","text":"<p>As well as how many CPUs or GPUs to request you may need to request a certain amount of memory, otherwise you will be limited to 2GB per CPU requested. It is important to be accurate with how much memory you need, but it is more important to overestimate the amount. If you ask for too much the worst that will happen is that other users may have to wait a bit longer for code to run, but if you ask for too little your program will fail, and you will have to resubmit your job and other users will may have to wait for your code to rerun anyway! These resources are requested with a line like this for a request per CPU requested: <pre><code>#SBATCH --mem-per-cpu 4G\n</code></pre> or just for the total amount <pre><code>#SBATCH --mem 4G\n</code></pre> You are also encouraged to specify how long your job might take - like the memory request this should always overestimate so that your job will not be killed. You can request a maximum of two days runtime, and if you specify no time you will be allocated a default of 12 hours after which your job will be ended, but if you specify a time otherwise it helps Slurm slot your job in to its queue during busy periods. You can do this with a request like <pre><code>#SBATCH --time 12:00:00\n</code></pre></p>"},{"location":"clusters/dias/#python","title":"Python","text":"<p>You may wish to use Miniforge to create Python environments. </p>"},{"location":"clusters/dias/#jupyter","title":"Jupyter","text":"<p>You can use Jupyter notebooks using a version of the script found at /share/apps/anaconda/jupyter-slurm.sh. You should make a copy, take a look through it, and alter parameters at the start as needed for a Slurm script, e.g. changing the partition to use a GPU. Also note the activation of Anaconda towards the end - you may want to activate your own environment here. The script will start Jupyter on one of the nodes, and give you connection instructions to connect your local machine to it (open the jupyter-notebook-xxx.log file created). These assume the use of Linux or Mac style command line ssh. If you are using PuTTY for example, you will need to use the port numbers and the node name in the port forwarding configuration of PuTTY.</p> <p>For example, at the top of your log file you will see a line like To connect: <pre><code>ssh -N -L 8350:compute-0-1:8350 eme@dias.hpc.phys.ucl.ac.uk\n</code></pre> If using PuTTY you need to connect to dias, port forwarding to host compute-0-1 with local and remote ports both 8350.</p> <p>Important: You should use scancel to end the Jupyter job when you are done with the session, or it will continue to run and block resources.</p> <p>The job number can be found in the output of squeue, and it is also the number in the appropriate jupyter-notebook-xxx.log filename.</p>"},{"location":"clusters/group/","title":"Research Group Clusters","text":"<p>Some of the research groups within the Department have their own dedicated computing resources. You should look at the pages for your research group for details, or contact your supervisor or group IT system manager.</p>"},{"location":"clusters/hypatia/","title":"Hypatia","text":""},{"location":"clusters/hypatia/#overview","title":"Overview","text":"<p>The Hypatia cluster supports workloads that are not suitable for DIAS because they require:</p> <ul> <li>multiple compute nodes, linked by high-bandwidth, low-latency network connections; or</li> <li>GPU nodes.</li> </ul> <p>It consists of two sets of hardware:</p> <ul> <li>RCIF nodes and storage, which were bought using strategic funds and are open for broad use subject to oversight detailed below;</li> <li>Cosmoparticle nodes and storage, which are attached to specific grants and therefore only available by specific users.</li> </ul>"},{"location":"clusters/hypatia/#account-creation-and-support","title":"Account creation and support","text":""},{"location":"clusters/hypatia/#rcif-hardware","title":"RCIF hardware","text":"<p>To ensure the new equipment is used to its best potential, we stated in the bid that access would be limited to those with existing HPC experience. This can be demonstrated through:</p> <ul> <li>successful completion of a previous project using HPC; or</li> <li>completion of a basic level of training provided by UCL or other provider (such as DIRAC). </li> </ul> <p>In case the facility becomes oversubscribed, a small users\u2019 committee will determine resource allocation, prioritising against the use cases given in the initial RCIF bid.</p> <p>For new users, training, and early development, other resources are more appropriate. </p> <p>In case the facility becomes oversubscribed, a small users\u2019 committee will determine resource allocation, prioritising against the use cases given in the initial RCIF bid. The users\u2019 committee will consist of</p> <ul> <li>Benjamin Joachimi (chair)</li> <li>Edd Edmondson (technical lead)</li> <li>Giovanna Tinetti (Group A rep)</li> <li>Jim Dobson (HEP rep)</li> <li>Sergey Yurchenko (AMOPP rep)</li> </ul> <p>If you would like access please email Edd and Benjamin, stating briefly your HPC experience and intended use for the RCIF facility. Students should arrange for a brief statement of support to also be sent by the supervisor. </p> <p>The RCIF segment has 4 nodes with 40 cores and 384GB RAM each. The compute queue for this hardware is 'RCIF', and requires users to be a member of the \u2018RCIF\u2019 group (see below for access oversight). RCIF users must remember to submit to the RCIF queue in every batch job - it is not the default.</p> <p>240TB of hybrid SSD/HDD storage, 1PB of HDD storage, and further compute nodes have been added from RCIF funding. Access to this is separate from other users of Hypatia from the Cosmoparticle Initiative and users must have access approved (and RCIF users will not also automatically get access to the other Hypatia compute and storage facilities).  Hypatia's GPU facilities (see below) also come under RCIF or CDT access requirements but have a separate partition for use.</p>"},{"location":"clusters/hypatia/#cosmoparticle-hardware","title":"Cosmoparticle hardware","text":"<p>The cosmoparticle hardware is overseen by PIs of the relevant grants which enabled its purchase. Potential users should ask their supervisor/line manager whether they may be able to gain access.</p> <p>The Cosmoparticle segment has currently has operational 6 nodes with 24 cores each and 256GB RAM, 4 cores with 40 cores each and 320GB RAM, 19 nodes with 64 cores and 512GB RAM, one 'SMP' node with 60 processors and 1.5TB RAM, and one with 80 processors and 1.5TB RAM. The compute queue for this hardware is 'cores64', \u2018cores40\u2019, \u2018cores24\u2019, \u2018smp\u2019, or \u2018compute\u2019 to access all non-SMP nodes.</p>"},{"location":"clusters/hypatia/#storage","title":"Storage","text":"<p>There are several additional storage areas for RCIF and Cosmoparticle use beyond the home directories. These areas are not backed up. Additionally, the fastest storage available is individual to each computation node and is that node's local disk. These are usable at /state/partition1 and are about 1TB each, 3TB on the SMP nodes, 2TB on 40 core nodes, and 15TB on GPU nodes. A job that does a lot of reading and writing can use this for the best speed during computations, and you can copy data back and forth from this area to a shared area during your job if needed. When your job has finished, it should tidy up after itself, and data in this area may be deleted without warning when jobs are not running.</p> <p>The storage areas are as follows - note that at this time /share/data1 is somewhat slower due to being over the login node which is not currently on Infiniband.</p> mountpoint available to size /share/hypatia Cosmoparticle 64TB /share/data1 Cosmoparticle 9TB /share/data2 Cosmoparticle 90TB /share/rcifdata RCIF 240TB /share/lustre RCIF 1PB <p>These areas are not backed up (although they have resilience against disk failures). These are a good storage location for data to be seen by all nodes, and which can be recomputed or redownloaded if necessary. When storing data in these areas please do so in a directory named after your username.</p> <p>Note also that the Lustre area while large can be crippled by having very large numbers of small files in it. Please avoid creating data like this and try to consolidate into smaller numbers of large files.</p> <p>See the 'Networking and external transfers' section below for how to move data from outside locations to these volumes.</p>"},{"location":"clusters/hypatia/#access","title":"Access","text":"<p>Login is by SSH to hypatia-login.hpc.phys.ucl.ac.uk.</p>"},{"location":"clusters/hypatia/#using-the-cluster","title":"Using the cluster","text":"<p>Hypatia runs the Rocks 7.0 Linux distribution and uses Slurm as a job manager. </p> <p>The home area has total capacity of about 9TB, which is backed up nightly. Although this has a decent amount of space, please minimise usage of it in order to help with backups and keep space free for new users. Alternative storage areas are described below. The home area is also the slowest tier of storage on Hypatia (although still quite fast).</p> <p>If you are unfamiliar with HPC environments and using batch submissions to run tasks please contact Edd for advice.</p>"},{"location":"clusters/hypatia/#job-submission","title":"Job submission","text":"<p>Partition (Queue) names:</p> <ul> <li>RCIF (RCIF 40 core nodes)</li> <li>COMPUTE (all cosmoparticle nodes)</li> <li>CORES24 (cosmoparticle 24 core nodes)</li> <li>CORES40 (cosmoparticle 40 core nodes)</li> <li>CORES64 (cosmoparticle 64 core nodes)</li> <li>SMP (cosmoparticle 60+80 core nodes)</li> </ul> <p>Useful links:</p> <ul> <li>Slurm manual: https://slurm.schedmd.com</li> <li>Slurm at COSMA (Durham, DiRAC): https://www.dur.ac.uk/icc/cosma/support/slurm/</li> <li>Slurm cheatsheet: https://slurm.schedmd.com/pdfs/summary.pdf</li> <li>Slurm 'Rosetta Stone' (for converting from the previous PBS/Torque): https://slurm.schedmd.com/rosetta.pdf</li> </ul>"},{"location":"clusters/hypatia/#alerting-other-users-when-submitting-large-numbers-of-jobs","title":"Alerting other users when submitting large numbers of jobs","text":"<p>Most of the time Hypatia runs below full capacity, which is part of its design: we want everyone to be able to access medium-level computing resources flexibly and at short notice.</p> <p>Please alert other users if you are queueing a large number of simultaneous runs, i.e. that is likely to use &gt;~50% of a given node type at a time.</p> <p>You can do this by sending an email to the hypatia-users mailing list, so that people can either plan around your usage or \u2014 if they have an urgent need for resource \u2014 they can request that you throttle back. (You should be subscribed to this list on creation, and a link to the mailing list page is given on login)</p>"},{"location":"clusters/hypatia/#gpus","title":"GPUs","text":"<p>Hypatia has six very powerful GPU nodes available to RCIF users and CDT users.</p> <p>The first four nodes have 384GB RAM, 15TB of very fast NVMe SSD storage, and 24 Intel Xeon Silver 4214 cores. Two of these nodes have 10 V100 GPUs, each capable of 7 Tflops double precision, 14 Tflops single precision, and 112 Tflops half precision, with 16 GB RAM on the GPU. Two further nodes have 5 and 4 A100 GPUs each capable of 9.7 Tflops double precision, 19.5 Tflops single precision, and 312 Tflops half precision, and either 40 GB or 80 GB memory (4x 80 GB, and 4x 40 GB plus 1 80 GB).</p> <p>A fifth node has 4 80 GB A100 GPUs connected by NVLink, which much upgrades the communication between the GPUs and particularly suits multiple GPU work. It has 512GB RAM and 32 Xeon Silver 4314 cores.</p> <p>The sixth node has 64 AMD EPYC cores, 1.5TB memory, and 10 L40S cards. The L40S cards each have 48GB on board memory, and for many tasks are comparable in speed to an A100 card but have poor double precision performance.</p> <p>The A100 cards are very popular due to their higher speed, memory capacity and improved feature set. As a result of their popularity queue times to get on to A100 cards can be quite long. However, the V100 cards are still very capable, and are sufficient for many tasks. Use of the V100 cards is encourage to alleviate pressure on the A100 cards, so if your job doesn\u2019t require the memory or features of the A100 please consider using V100s instead. The slight increase in job times will often be compensated by reduced queuing and this makes the cluster more efficient for everyone. The L40S cards as stated above are also excellent for many tasks, but will be very slow for double precision calculations so it's best to do small test runs on these cards first to see if they suit your code.</p> <p>Further Grace Hopper systems are due to be installed soon, which will expand these options further.</p> <p>The GPU nodes have CUDA installed natively, and Apptainer (formerly Singularity) is available if you need to run a containerised OS (contact Edd if you'd like more info on this). The nodes are accessed by the GPU partition. The GPUs are requested through Slurm Generic Resource (GRES) requests. The CUDA_VISIBLE_DEVICES will be suitably set to restrict you to the GPUs allocated. To specify a type of GPU you can either do this in the gres field, e.g. <pre><code>#SBATCH --gres=gpu:a100:1\n</code></pre> or in the constraints field (which allows multiple types) e.g. <pre><code>#SBATCH --gres=gpu:1\n#SBATCH --constraint='a100|l40s'\n</code></pre> The very fast NVMe storage is available per node in /state/partition1, with 15TB per node (see Storage, above). This is a comparatively small but extremely fast scratch area - please only keep data on it while working on data on a node and remove it to larger volumes afterwards. It\u2019s acceptable to keep data in the area between jobs if it will only be up to a few days between submissions, but old data may be cleared out if left untouched (we will try to contact you first).</p> <p>For information on running containers (e.g. to use Tensorflow on the GPUs) please see Containers and Tensorflow-GPU usage on clusters.</p>"},{"location":"clusters/hypatia/#example-job","title":"Example job","text":"<pre><code>#!/bin/bash\n#SBATCH -p GPU\n#requesting one node\n#SBATCH -N1\n#requesting 12 cpus\n#SBATCH -n12\n#requesting 1 V100 GPU\n#SBATCH --gres=gpu:v100:1\n#SBATCH --mail-user=e.edmondson@ucl.ac.uk\n#SBATCH --mail-type=ALL\necho $CUDA_VISIBLE_DEVICES\n./program \n</code></pre>"},{"location":"clusters/hypatia/#vscode-usage","title":"VSCode Usage","text":"<p>Visual Studio Code (VSCode) can cause problems on shared systems due to both high load and its tendency to search networked filesystems without prompting, clogging up the network on the login node. VSCode processes may be killed without warning, and the system is set to prioritise these for killing if memory resources become overloaded. Please take the time to set the files.watcherexclude, search.exclude, and search.include parameters to exclude anything except the specific source code trees you are working on so it does not try to search through large amounts of data over the network. Also please disable search.followsymlinks. This documentation provides details of how to set such parameters as does this blog post.</p>"},{"location":"clusters/hypatia/#software","title":"Software","text":"<p>As well as the system default software, some software is available under the modules system which lets you dynamically alter your environment to use specific versions of software as needed. Edd is happy to add software to this as needed, and otherwise software can be installed in your own user areas located in shared areas (such as /home and /share/hypatia), and then be used across the cluster. If you wanted to load the boost/1.67.0 package for example, just do 'module load boost/1.67.0' or 'module load boost/1.67.0', and remember to do this in any job submission scripts needing it.</p>"},{"location":"clusters/hypatia/#python","title":"Python","text":"<p>You may wish to use Miniforge to create Python environments. </p>"},{"location":"clusters/hypatia/#jupyter-notebooks","title":"Jupyter notebooks","text":"<p>Running Jupyter notebooks requires you to unset the environment variable XDG_RUNTIME_DIR. A suggested Slurm script is available here: jupyter-slurm.sh</p> <p>Submit the script with sbatch, modifying SBATCH directives if required, and check the jupyter-notebook-xxx.log files for output containing ssh connection information and notebook tokens (full output can take a minute or two to write out).</p>"},{"location":"clusters/hypatia/#mpi-tasks","title":"MPI tasks","text":"<p>The currently recommended MPI module is mpi/mvapich/2-2.3.1 but note that the recommended way to run an MPI job with this module in a Slurm job is something like: <pre><code>module load mpi/mvapich/2-2.3.1\nsrun --mpi=pmi2 ./mpiprogram \n</code></pre> OpenMPI is also available. To use it load the module mpi/openmpi/4.0.1.</p>"},{"location":"clusters/hypatia/#openmp-jobs","title":"OpenMP Jobs","text":"<p>To avoid issues with processor affinity restricting you to running on one core, OpenMP jobs should have the OMP_NUM_THREADS variable set, then be run in the script via 'srun -n1'. Without the srun command, access to the allocated processors will not be granted. For example, do something like <pre><code>#!/bin/bash\n#SBATCH -p CORES24\n#SBATCH -N1\n#SBATCH -n24\ncd ~/openmpcode\nexport OMP_NUM_THREADS=$SLURM_JOB_CPUS_PER_NODE\nsrun -n1 ./openmp_executable \n</code></pre></p>"},{"location":"clusters/hypatia/#interactive-jobs","title":"Interactive jobs","text":"<p>A simple interactive job can be run by using srun to call a shell. srun will make the necessary resource request.  <pre><code>srun --pty bash\n</code></pre> Further arguments can be used to request additional resources beyond just one processor; for example, to run an interactive job on CORES40 using 40 processors, run: <pre><code>srun -p CORES40 -N1 -n40 --pty bash\n</code></pre></p> <p>For X11 applications, you can add '--x11' as a flag. </p>"},{"location":"clusters/hypatia/#networking-and-external-transfers","title":"Networking and external transfers","text":"<p>The default network used is ethernet (1 gigabit/s). This handles standard traffic between the nodes, the home filesystem traffic, and connections to the outside world.</p> <p>Infiniband is also available providing performance at around 100 gigabit/s. MPI jobs can use Infiniband natively, and there is also IP over Infiniband available by using the .mlnx suffix to node names (e.g. compute-0-4.mlnx).</p> <p>There are two external interfaces through which all traffic to the outside world must flow. One is a direct connection to the login node, and the other is on the head node which all other nodes make use of. These are 1 gigabit/s. As a result of these bottlenecks, large file transfers are better off using the head node's interface so that interactive use of the login node is not impacted. To do this, it's recommended you log in by ssh to the following nodes to move in or out large amounts of data (small copies are fine over the login node). These nodes allow ssh access without needing a job running on them (unlike compute nodes) but please do not run anything on them except file transfers as they are also responsible for running filesystem exports and we don't want to load them unnecessarily or risk bringing one down by accident.</p> data area transfer node /share/hypatia nas-0-0 /share/data1 login node /share/data2 nas-0-0 /share/rcifdata nas-0-1 /share/data3 nas-0-2 <p>As Hypatia is firewalled from the outside world, to do a large transfer via zuserver1 or another UCL gateway machine use a command similar to this: <pre><code>rsync -ave 'ssh -o \"ProxyCommand ssh username@zuserver1.star.ucl.ac.uk exec nc %h %p\"'filename username@hypatia-login.hpc.phys.ucl.ac.uk:/location/\n</code></pre></p> <p>Note that the Lustre area does not have a transfer node. Contact Edd for advice on transferring to this area.</p>"},{"location":"clusters/hypatia/#acknowledgements-in-publications","title":"Acknowledgements in publications","text":"<p>For RCIF hardware we recommend adding the following acknowledgement to publications:</p> <p>This work used computing equipment funded by the Research Capital Investment Fund (RCIF) provided by UKRI, and partially funded by the UCL Cosmoparticle Initiative.</p> <p>For work using Cosmoparticle facilities:</p> <p>This work used facilities provided by the UCL Cosmoparticle Initiative.</p>"},{"location":"cmmp/","title":"Condensed Matter and Materials Physics (CMMP)","text":"<p>For IT support in the CMMP group please contact the LCN support team at lcn.it-support@ucl.ac.uk.</p>"},{"location":"hep/","title":"High-Energy Physics (HEP)","text":"<p>For IT support in the HEP group please e-mail support@hep.ucl.ac.uk.</p>"},{"location":"hep/#hep-computing-information","title":"HEP computing information","text":"<p>Information for members of the HEP group can be found on the HEP Computing Wiki. If you don't already have access you should click <code>TWikiRegistration</code> on the login page and fill in the registration form. Registrations are dealt with by the support team so you will have to wait a while for us to approve your account. To help us deal with your application quickly it helps if you include:</p> <ul> <li>your UCL e-mail address if you have one;</li> <li>a brief statement of your role and/or supervisor in the \"comment\" field so we know you are part of the HEP group.</li> </ul>"},{"location":"hep/#new-accounts","title":"New accounts","text":"<p>To request a new account, e-mail support@hep.ucl.ac.uk stating:</p> <ul> <li>the full name and e-mail address of the person needing the account;</li> <li>the role of the account holder (e.g. new academic or postdoc, PhD student, project or summer student);</li> <li>the supervisor or line manager of the account holder;</li> <li>how long the account is needed for (typically a year for a project student, longer for others, but can be extended later);</li> <li>the preferred user name (generally the user's surname, or initial(s) + surname);</li> <li>any groups (e.g. atlas, nemo...) the account should be added to (these can also be added later).</li> </ul>"},{"location":"teaching/labs/","title":"Teaching Labs","text":"<p>The PCs in the three teaching labs in the Physics Building are mostly installed with the UCL Windows environment Desktop@UCL. The same software available on these computers can also be used on other Desktop@UCL PCs on campus, and remotely via the Desktop@UCL Anywhere service. See Computers on the ISD web site.</p> <p>Some computers in the teaching labs are set up differently or have additional software installed for use with specific experiments. This will be explained to you by the staff running the relevant lab course.</p>"},{"location":"teaching/linux/","title":"Linux clusters","text":"<p>The department has a Linux cluster with CPU and GPU nodes, which is available for the use of students carrying out MSc or final-year MSci projects: DIAS.</p> <p>Some project students may also have access to research group Linux computing resources through their supervisor.</p>"},{"location":"teaching/uclo/","title":"UCL Observatory (UCLO)","text":"<p>The computer systems available at the observatory are documented on the UCLO web site under Computing support. More detailed information for students is available on the Moodle pages for the relevant modules.</p>"},{"location":"training/","title":"Computing Guides and Training","text":"<p>This page provides a list of resources that may be useful in learning some of the general concepts and skills that you may need in your computational work.</p>"},{"location":"training/#linux","title":"Linux","text":"<p>Linux is an operating system that is used by most of the computing clusters used in scientific research. To use any of the clusters in the department and most clusters elsewhere you will need to know the basics of Linux, and in particular how to use the command line (the shell) instead of a graphical user interface (GUI) to interact with the system.</p> <p>A good starting point is the Unix Shell lesson designed by Software Carpentry. This is designed to be taught as part of a Software Carpentry workshop. These are run several times by ARC: see the ARC training web page. There is also a self-paced Moodle course available: ARC - Introducing the Unix Shell.</p>"},{"location":"training/#ssh","title":"SSH","text":"<p>SSH (Secure Shell) is a protocol (way of communicating) and associated software package that enables you to interact with a computer system without being physically in front of it. You will need to use this mechanism to connect (log in) to a Linux computer or cluster: it creates a connection between the local machine (the one in front of you) and the remote one, sends the input you type and displays the output you get back.</p> <p>A detailed guide is available on the SSH Academy web site.</p>"}]}